{"To evaluate the proposed distant meta-path similarities, we develop two text-based HINs (i.e., 20NG-HIN and GCAT-HIN) based on benchmark text datasets, i.e., 20Newsgroups  and RCV1 .": [[158, 170, "DATASET"], [176, 180, "DATASET"]], "GCAT in RCV1 dataset.": [[8, 12, "DATASET"]], "The RCV1 dataset contains manually labeled newswire stories from Reuter Ltd .": [[4, 8, "DATASET"]],
	"YAGO) that maintain probabilistic (temporal) facts and inference rules.": [[0, 4, "DATASET"]],
	"Information extraction allows automated construction of big knowledge graphs (KGs) such as Google's Knowledge Vault , NELL , YAGO , DeepDive  and ReVerb .": [], "e last two datasets, MQ2007 and MQ2008, were based on the Million ery Track  and consist of 1,700 and 800 queries, respectively, but have far fewer assessed documents per query.": [[21, 27, "DATASET"], [32, 38, "DATASET"]], "When |R | = 15 as displayed in , we see a single case where PPM performs worse than a previous method: on MQ2007 under the perfect click model SOSM performs signi cantly be er than PPM.": [[106, 112, "DATASET"]], "the OHSUMED collection, the MQ2007 collection and the MQ2008 collection.": [[28, 34, "DATASET"], [54, 60, "DATASET"], [4, 11, "DATASET"]], "Lastly, the two most recent datasets MQ2007 and MQ2008 were based on the Million ery Track  and consist of 1700 and 800 queries, respectively, but have far fewer assessed documents per query.": [[37, 43, "DATASET"], [48, 54, "DATASET"]], "To evaluate the user experience, the online performance of C-MGD and MGD can be examined in     748.1  746.9  NP2004 719.9  769.8  781.8  737.8  740.5  TD2004 298.9  268.1  267.6  295.2  296.4  MQ2007 412.5  448.4  443.1  423.4  421.1  MQ2008 523.2  547.3  543.1  531.3  527.1  MSLR-WEB10k 336.6  347.9  351.2  340.7  342.1  OHSUMED 494.8  483.5  483.2  494.4  495.4  Yahoo 732.1  773.7  778.6  741.5    757.0  655.7  657.5  TD2003 251.6  247.3  248.8  257.0  255.3  HP2004 616.1  697.9  718.7  652.6  651.8  NP2004 617.8  719.0  736.2  661.9  663.3  TD2004 245.0  232.8  237.0  (Column 2 vs. 5 and 6).": [[37, 43, "DATASET"], [56, 58, "DATASET"], [110, 116, "DATASET"], [152, 158, "DATASET"], [271, 276, "DATASET"], [236, 242, "DATASET"], [283, 289, "DATASET"], [325, 332, "DATASET"], [368, 373, "DATASET"], [425, 431, "DATASET"], [467, 473, "DATASET"]], "Only for MQ2008 under the informational user model this di erence is greater than 0.1 NDCG.": [[5, 8, "DATASET"]], "FB15k  and YAGO43k  are subsets of Freebase and YAGO, respectively.": [[0, 5, "DATASET"], [11, 18, "DATASET"]], "We also collected entity types that are mapped to entities in both datasets from  for FB15k and the YAGO taxonomy 1 for YAGO43k.": [[86, 91, "DATASET"], [100, 104, "DATASET"], [120, 127, "DATASET"]], "Currently, a number of knowledge graphs, such as WordNet, YAGO, DBpedia and Freebase, have been manually or automatically constructed.": [[58, 62, "DATASET"]], "To evaluate the performance of our model, we conducted experiments using two LETOR benchmark datasets : Million ery Track 2007 (MQ2007) and Million ery Track 2008 (MQ2008).": [[77, 82, "DATASET"], [128, 134, "DATASET"], [164, 170, "DATASET"]], "Both datasets use the GOV2 collection which includes 25 million Web pages in 426 gigabytes.": [[22, 26, "DATASET"]], "As we can see, there are 1692 queries on MQ2007 and 784 queries on MQ2008.": [[41, 47, "DATASET"], [67, 73, "DATASET"]], "e average number of relevant page per query is about 10.3 and 3.7 on MQ2007 and MQ2008, respectively.": [[69, 75, "DATASET"], [80, 86, "DATASET"]], "For example, the relative improvement of ViP over ViP CNN on query-dependent snapshot is about 3.9% and 3.1% in terms of P@1 and NDCG@1 on MQ2007, respectively.": [[139, 145, "DATASET"]], "For example, On MQ2008 dataset, the relative improvement of our ViP model against the best-performing baseline (i.e.": [[16, 22, "DATASET"]], "shows two candidate Web pages of the query \"rusty black bird\" in MQ2008 dataset, which have totally di erent layouts.": [[65, 71, "DATASET"]], "e page (A) (DocId: GX059-61-15727287 in GOV2 corpus) shows a list of vertebrate animal species list, and is labeled as Non-Relevant to the query.": [[40, 44, "DATASET"]], "e page (B) (DocId: GX095-93-12495293 in GOV2 corpus) describes the rusty black bird in detail, and is labeled as Highly Relevant to the query.": [[40, 44, "DATASET"]], "Many popular knowledge graphs such as Freebase, YAGO or DBPedia maintain a list of non-discrete a ributes for each entity.": [], "Examples of popular large scale knowledge graphs include YAGO, Freebase and Google Knowledge Graph.": [], "1.783874 Location : Sample non-discrete attributes and their values on Freebase and YAGO.": [], "Additionally, we include an estimate on the number of parameters on the YAGO subgraph that we use in our experiments.": [], "\u2022 YAGO  is a semantic knowledge base that aggregates data from various sources including WikiPedia.": [], "YAGO contains many famous places and people which naturally contain many a ribute values such as height, weight, age, population size, etc.": [], "We construct a subset of YAGO by removing entities that appear less than 25 times and further ltered away entities that do not contain a ribute information.": [], "Using data and taxonomy graphs from  [1] and YAGO [2], we have conducted an experiment on the percentage of pattern graphs that have non-empty match results.": [], "YAGO) via taxonomy simulation; no patterns with 6 or more nodes can identify matches.": [], "(2) YAGO [2] consists of (i) a data graph with 5.13M nodes and 5.39M edges; and (ii) a built-in taxonomy graph (forest) with 6488 concepts (nodes), with average height 3.27 (maximum height 13).": [], "Varying |V Q | of Q from 2 to 10, we report the results on DBpedia and YAGO in .": [], "We found that both TSim and gsim nd very few matches for medium-sized patterns on DBpedia and YAGO, e.g., when |V Q | \u2265 6, they both cannot identify any matches, although TSim can identify more matches than gsim on small patterns, e.g.,TSim found 1.4 times mores matches than gsim did when |V Q | is 4 on DBpedia.": [], "Using the accuracy measure, we inspected the match relations returned by TSim to the 30 patterns and found that their average accuracy is 0.98 and 0.94 on DBpedia and YAGO, respectively.": [], "Using the same setting as Exp-(2), we inspected match relations to the top-5 relaxed patterns of the 30 patterns, and found that the average accuracy on DBpedia and YAGO is 0.81 and 0.75, respectively, when topological ranking function is used, and is 0.72 and 0.68 with the diversi ed topological ranking function.": [], "Varying the number |V Q | of nodes in Q from 2 to 10, we report the results in Figures 7(a) and 7(b) for DBpedia and YAGO, respectively.": [], "(i) Even when \u00b5 is small, both evalTF and evalDF are able to nd sensible matches on the two datasets, and the quantities are larger than that of TSimC, e.g., they found 8525 and 16351 matches when \u00b5 = 1 on YAGO, respectively, compared to 5320 by TSimC.": [[145, 150, "DATASET"]], "(ii) Both evalTF and evalDF make more use of larger \u00b5 as they can take into account more information on the data graphs and taxonomy, e.g., evalTF and evalDF found 39478 and 76502 matches when \u00b5 = 5 on YAGO, while TSimC still found 5320 matches.": [], "For example, evalTF identi ed 10100 matches when k = 5 on YAGO, and even more for evalDF, while TSimC found no match.": [], "The accuracy is consistently above 85% and 88% on DBpedia and YAGO, respectively, even when M = 0; and is above 99% when M is above 2 on both datasets.": [], "YAGO).": [], "This is because TSimTF and TSimDF encode the dataset information into the ranking functions, such that they both nd many more meaningful matches than TSimC, while TSimC always returns empty results even with medium size patterns, e.g., queries Q on YAGO with |V Q | = 6.": [], "Using the same setting as in Exp-(4) with M = 4 by default, we evaluated the e ciency of our explanation algorithms expTF and expDF on DBpedia and YAGO.": [], "(b) Its average evaluation time of the top-k relaxed patterns is 1.8 times faster than direct evaluation, and the gap grows bigger with larger k. (c) It can explain matches to the relaxed patterns accurately and e ciently, e.g., it explains relaxations with accuracy above 85% even without access to the data graphs, and achieves 99% accuracy when two data accesses are allowed on both DBpedia and YAGO, in 2.4s.": [], "LETOR4.0 dataset contains two separate data sampled from the .GOV2 corpus using the TREC 2007 and TREC 2008 Million ery track queries, denoted as MQ2007 and MQ2008, respectively.": [[61, 66, "DATASET"], [84, 93, "DATASET"], [146, 152, "DATASET"], [157, 163, "DATASET"]], "MQ2007 is a bit larger, which contains 1692 queries and 65,323 documents.": [[0, 6, "DATASET"]], "While MQ2008 only contains 784 queries and 14,384 documents.": [[6, 12, "DATASET"]], "Since the query number in MQ2008 is too small, which will cause the serious insu cient training problem for deep learning models, we propose to merge the training set of MQ2007 to that of MQ2008.": [[170, 176, "DATASET"], [26, 32, "DATASET"]], "en we form a new large data set, still denoted as MQ2008.": [[50, 56, "DATASET"]], "On DBpedia and YAGO, we mined a total of 88 R-equivalence rules.": [[3, 10, "DATASET"], [15, 19, "DATASET"]], "These results beg a somewhat obvious question namely: Why should structured topic representations such as TH 1 and TH 2 necessarily outperform flatter representations on an MDS task like the one featured in DUC 2004?": [[173, 176, "DATASET"]], "In total, MQ2007 and MQ2008 contains 69,623 and 84,834 query-document pairs, respectively.": [[10, 16, "DATASET"], [21, 27, "DATASET"]], "From the results on MQ2007 and MQ2008, we can see that: 1) None of existing deep learning models could perform comparably with learning to rank methods.": [[20, 26, "DATASET"], [31, 37, "DATASET"]], "DRMM) on MQ2007 is 16.1% w.r.t.": [[9, 15, "DATASET"]], "LambdaMart) on MQ2007 is 7.0% w.r.t.": [[15, 21, "DATASET"]], "erefore, we conduct a detailed analysis on MQ2007 to show the comparisons of DeepRank with di erent se ings, with expect to give some insights for implementation.": [[43, 49, "DATASET"]], "In this section, we conduct experiments on MQ2007 to compare the three di erent versions of DeepRank, denoted as DeepRank-DNN, DeepRank-CNN, and DeepRank-2DGRU.": [[43, 49, "DATASET"]], "As for the reason why reciprocal and exponential function performs be er than linear function, we think this is because MQ2007 is extracted from GOV data, where title and abstraction information may play a dominant role in determining the relevance.": [[120, 126, "DATASET"], [145, 148, "DATASET"]], "Using an evaluation over DUC 2005-2007 benchmarks, our summarization approach was shown to provide be er summarization quality compared to state-of-the-art unsupervised query-focused multi-document summarization methods .": [[25, 38, "DATASET"]], "Experiments on TREC GOV2 data collection show that flat position index can reduce the index size and speed up phrase querying substantially, compared with traditional word-level index.": [[15, 24, "DATASET"]], "In the evaluation, the results obtained by using different approaches (system-generated summaries) are compared with summaries created by humans (reference summaries) using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics  as ROUGE metrics are widely used by the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) for update summarization task ).": [], "On the data of DUC 2006, our system performs worse than only S24 but much better than other systems on ROUGE-2.": [[15, 23, "DATASET"]], "Relevance assessment tasks measure the impact of summarization on determining the relevance of a document to a topic ; these have been used in many large-scale extrinsic evaluations, e.g., the Tipster SUMMAC evaluation  and the Document Understanding Conference (DUC) .": [], "The first step involves zeroing out an array of about 25, 000 numbers in the case of block size 2 10 docIDs on GOV2, and modern CPUs can do this in a few microseconds.": [[111, 115, "DATASET"]], "In addition to the Indri search engine over clueweb09 dataset, a third party search engine (Microsofts Bing) is also used as an alternative document searcher.": [[44, 53, "DATASET"]], "The TREC Web collections WT2g or WT10g  provide a way to partition documents by different Web servers.": [[4, 28, "DATASET"], [33, 37, "DATASET"]], "As both TREC and NTCIR use the top answer accuracy as an evaluation metric to evaluate factoid questions, we used the top answer accuracy to compare the performance.": [], "For example, The property dbpedia:phylum defined by dbpedia.org and the property gs:inPhylum defined by geospecies.org have the same intensional meaning: their values both refer to the same taxonomic rank in biology when given a certain instance (species).": [], "A document collection and information needs of the NTCIR Lifelog test collection are highly individual and multimodal when compared to conventional test collections.": [[51, 64, "DATASET"]], ": Results over AQUAINT, GOV2, and ClueWeb-B, using 36, 150, and 50 queries, respectively.": [[15, 22, "DATASET"], [24, 28, "DATASET"], [34, 43, "DATASET"]], "Our evaluation is mainly performed on the GOV2 collection.": [[42, 46, "DATASET"]], "TREC10 selected 84 topic categories for evaluations, after removing those that have zero or one positive example only in the first portion of RCV1 documents, and those that have more than 5% of the documents in that portion .": [[0, 6, "DATASET"], [142, 146, "DATASET"]], "show the relative cost of different approaches to on-the-fly construction for index lists that are  : Effectiveness (MAP) on GOV2 for topics 701-850.": [[125, 129, "DATASET"]], "In ROBUST and WT10G, the average lengths of top-retrieved documents in VN-DP are lower than those in DP, whereas in GOV2 and ClueWeb09 the average lengths in VN-DP are higher than those in DP.": [[3, 9, "DATASET"], [14, 19, "DATASET"], [116, 120, "DATASET"], [125, 134, "DATASET"]], "In particular, for the recent TREC Terabyte track, using the GOV2 collection (25 million documents), the completeness of the relevance judgement is two orders of magnitude less than in the early TREC years.": [[61, 65, "DATASET"]], "Test collections such as OHSUMED, Reuters-21578, 20NG, and RCV1 have been important resources for the evaluation of automatic text categorization (TC) technologies.": [[25, 32, "DATASET"], [34, 47, "DATASET"], [49, 53, "DATASET"], [59, 63, "DATASET"]], "We notice that only ROUGE recall scores on DUC 2005 are provided in that paper.": [[43, 51, "DATASET"]], "YAGO has been successfully used in many knowledge-oriented systems, such as the YAGO-NAGA project , and most prominently in IBM's Watson , a system for answering Jeopardy questions.": [], "We used the standard RCV1 collection of documents  for our experiments.": [[21, 25, "DATASET"]], "We utilized the human summaries from DUC 2006 to generate the training data and used it to learn a sentence scoring function with Support Vector Regression.": [[37, 45, "DATASET"]], "To overcome this, an allowance of 100 characters per nugget match was introduced at the TREC QA track; the NTCIR ACLIA task determined the allowance parameters based on average nugget lengths .": [], "The YAGO ontology has a high level of manually confirmed accuracy.": [], "Based on three benchmark datasets, namely, RCV1, R8, and WT10G under the context of information filtering (IF) and information retrieval (IR), our rigorous experiments show that the proposed ATS model can effectively identify relevant topics with respect to users' specific interests, and hence to improve the performance of IF and IR.": [[43, 47, "DATASET"], [49, 51, "DATASET"], [57, 62, "DATASET"], [191, 194, "ALGORITHM"]], "We leave the analysis of the TREC runs using true relevance assessments for the NTCIR intents for future work.": [], "To further prove this result, we extend the experiment to the DUC 2006 and the 2007 data sets.": [[62, 83, "DATASET"]], "The authors have reported that the use of word embeddings, trained globally, improves retrieval performance on AP, Robust and GOV2.": [[111, 113, "DATASET"], [115, 121, "DATASET"], [126, 130, "DATASET"]], "Distributed Information Retrieval (DIR), also known as Federated Search, integrates multiple searchable collections and provides direct access to them through a unified interface .": [], "(Q2) {\"Isabel_Sanford\", \"Nathan_Lane\"} on YAGO.": [], "For assessment the performance of the proposed method we used the document datasets DUC 2002 and corresponding 100-word summaries generated for each of documents.": [[84, 92, "DATASET"]], "suggests that, for the GOV2 collection, we have a Zipf exponent \u03b1 somewhere in the neighborhood of 1.2.": [[23, 27, "DATASET"]], "To this end, FreeQ makes use of some external ontologies, such as WordNet  and YAGO .": [], "In 2003 work was begun on an updated summarization roadmap  and the results of this effort and various other possibilities were discussed at DUC 2004.": [], "The YAGO knowledge-base provides a finegrained high-accuracy entity type categorization which has been constructed by combining Wikipedia category assignments with WordNet synset information.": [], "Retrieval equivalence in GOV2.": [[25, 29, "DATASET"]], "18 hours for 2 iterations) due to the use of domain-specific constraints for the reasoner, and also because of the larger number of seed facts obtained from YAGO for the first iteration.": [], "This is a reasonable evaluation strategy for summarising in general even if individual applications will tolerate illformed output; and it has been one strand in DUC.": [], "Specifically: 1) VN-LCE vs. LCE: the RIs of VN-LCE are higher than LCE in WT10G and ClueWeb09, whereas the RIs of VN-LCE and LCE are similar in ROBUST and GOV2.": [[74, 79, "DATASET"], [84, 93, "DATASET"], [144, 150, "DATASET"], [155, 159, "DATASET"]], "We perform all experiments on the GOV2 collection, consisting of 25,205,183 documents.": [[34, 38, "DATASET"]], "YAGO ontology has a comprehensive type system and it stores a rich set of attributes of entities.": [], "shows the distribution of \u03c1 values required to when targeting a MED RBP < 0.001, which aggressively targets no measurable difference in the results lists between exhaustive and aggressive Jass traversals.": [], "RCV1 dataset is a manually labeled newswire collection of Reuters News from 1996-1997.": [[0, 4, "DATASET"], [58, 70, "DATASET"]], "In this section, we compare the QA systems that incorporate our approach with other QA systems that participated in the recent TREC and NTCIR QA task.": [], "These evaluations, known as the Document Understanding Conferences (DUC), have organized yearly evaluations of automatically-produced summaries by comparing the summaries created by by systems against those created by humans.": [[68, 71, "VENUE"], [32, 66, "VENUE"]], "This is encouraging because the large number of queries automatically generated from WT10g (in the magnitude of 10 6 ) give us the opportunity to study in the future how the network can learn from past queries and evolve to improve the search performance over time.": [[85, 16, "DATASET"]], "The approaches used in DUC have been largely extractive, i.e., they have been mainly concerned with selecting appropriate sentences from the material to be summarized and ordering them, largely unchanged, to create the output summary.": [], "DUC 04 provided 50 document sets and four manual summaries of each set for its task 2.": [[0, 6, "DATASET"]], "The editors of the special issue, who are also the authors of this paper, are the three Japanese researchers co-organizing the Patent Retrieval Task in the NTCIR Workshop.": [], "The recall-precision performance curves of MEDLINE, CISI and FBIS are shown in Figs.": [[43, 50, "DATASET"], [52, 56, "DATASET"], [61, 65, "DATASET"]], "We therefore introduce a new test collection, based on the WT10g [3]: \u2022 WT10g-1000col-byUrl: The documents from the WT10g collection are divided into 11.653 collections based on their URLs and the 1000 collections with the largest number of documents were selected.": [[59, 64, "DATASET"], [72, 90, "DATASET"], [116, 120, "DATASET"]], "We used two publicly available datasets (MQ2007 & MQ2008).": [[41, 47, "DATASET"], [50, 56, "DATASET"]], "The NTCIR 1CLICK-2 test collection includes 100 Japanese and 100 English queries.": [[4, 18, "DATASET"]], "The RCV1 corpus is publicly available at http://about.reuters.com/researchandstandards/corpus/.": [[4, 8, "DATASET"]], "Although many tasks at TREC, CLEF or NTCIR have been devoted to query analysis and document retrieval, only a few of them particularly focused on temporal aspects in search.": [], "WT10g contains much more small size resources (totally 625 out of 934 resources contain less than 1,000 documents) than Trec123 (3 out of 100 resources contain less than 1,000 documents).": [[0, 5, "DATASET"], [120, 127, "DATASET"]], "WT10G is a small Web collection and GOV2 is a crawl of the .gov domain.": [[0, 5, "DATASET"], [36, 40, "DATASET"]], "Amongst the page based features, title was useful for recognizing navigational and transactional queries in WT10g.": [[108, 5, "DATASET"]], "All experiments were run on the wsj text file and the results were obtained with 99% confidence.": [[32, 35, "DATASET"]], "The NTCIR  project group has organized a series of workshops featuring ''distributed experiments with centralized evaluation\" every 18 months.": [], "The GOV2 corpus was used to construct 3 experimental federated search testbeds, varying the number of target collections: 1,000, 250, and 30.": [[4, 8, "DATASET"]], "The WT10g data was divided into 11,485 collections based on document URLs.": [[4, 9, "DATASET"]], "In each case, the subscript \"NC\" indicates non-contiguous posting lists Our experiments were conducted using the GOV2 text collection used in the TREC Terabyte track 2 .": [[113, 117, "DATASET"]], "The summarization module in QCS is based on the work of  and its implementation for the DUC 2003 evaluation .": [], "Two of these are the NTCIR  and CLEF  patent search tracks, which have examined ad-hoc search, invalidity search, and prior-art search.": [], "Standard Cranfield-type methods have been basically used to assess CLIR experiments in TREC, CLEF and NTCIR.": [], "In collaborative retrieval experiments such as NTCIR, there can be an extended interval between the submission of runs and the release of relevance assessments.": [], "Datasets: We use the following two large publicly available knowledge graph corpora for our evaluation: \u2022 YAGO Entity Relationship Graph (http://www.yago-knowledge.": [[106, 136, "DATASET"]], "We evaluate our methods with four corpora WT10g, TREC Blogs06 4 , SogouT 2.0 5 , and ClueWeb09-T09B 6 .": [[42, 46, "DATASET"], [49, 63, "DATASET"], [66, 77, "DATASET"], [85, 100, "DATASET"]], "The set of queries used in our experiments with WT10g is based on the one used by .": [[48, 53, "DATASET"]], "Recently, QA systems have been extended to cover cross-lingual questionanswering (CLQA), which accepts questions in one language (source language) and searches for answers from documents written in another language (target language).": [], "In the MQ2007 and MQ2008 datasets, every document is marked with a relevance label between 0 and 2, while the other datasets only have binary labels.": [[3, 6, "DATASET"], [18, 24, "DATASET"]], "YAGO  is the semantic representation of Wikipedia , WordNet  and GeoNames  comprised of 120,000,000 triples.": [[0, 4, "DATASET"]], "3 Apart from traditional and cross-lingual question answering more recently NTCIR launched a geo-temporal QA track addressing geographic and temporal aspects in information retrieval (see below).": [], "Not surprisingly, it has been experimentally shown across different systems that multiple disk accesses (e.g., 7 in GOV2) may be needed to retrieve a fragmented inverted list regardless of the list length .": [[116, 120, "DATASET"]], "For our DGCNN models, we set the number of selected nodes as 192, since from Table 1 we can see that on average there are more tokens in NYTimes than in RCV1 data.": [[8, 13, "ALGORITHM"], [137, 144, "DATASET"], [153, 157, "DATASET"]], "The original MCR algorithm is amongst the most accurate size estimation techniques at present, but is of limited use in DIR tools since it requires samples of fixed size.": [[13, 16, "ALGORITHM"]], "We used two datasets, TC1 (Section 4.1.2) and TC2 (Section 4.1.3).": [[22, 25, "DATASET"], [46, 49, "DATASET"]], "More specifically, for Okapi, the performance surfaces on GOV2 are shifted in the decreasing direction of b, relative to those of other collections.": [[58, 62, "DATASET"], [23, 28, "ALGORITHM"]], "As for the peers' content, we used two document collections in our experiments: 1) a subset of GOV2 3 consisting of 1,000,000 randomly selected documents, and 2) Reuters Corpus 4 (810,000 articles of news stories).": [[95, 101, "DATASET"], [162, 178, "DATASET"]], "\u2022 Manually translated summaries from DUC 2004 into Gujarati, that can be used to evaluate English to Gujarati CLS.": [], "Section 8 evaluates the performance of the full query processing mechanism on the 25.2 million pages from the TREC GOV2 collection.": [[110, 119, "DATASET"]], "It was demonstrated earlier, however, that the cluster hypothesis is not true for the CISI collection while it holds for the other collections to varying extents.": [[86, 90, "DATASET"]], "YAGO is very rich in terms of famous individuals and contains plenty of interesting facts about them.": [[0, 4, "DATASET"]], "In this paper we focus on the problem of enrichment of Freebase categories and entities with the conceptual categories of YAGO.": [[122, 126, "DATASET"]], "We used three corpora in our evaluation: Wikipedia  (6M documents 10 , 27 GB), Reuters RCV1  (0.8M documents, 2.5 GB), and GOV2  (25M documents, 426 GB).": [[41, 50, "DATASET"], [79, 91, "DATASET"], [123, 127, "DATASET"]], "NELL (NeverEnding Language Learning)  and YAGO  knowledge bases have not only steadily accumulated new knowledge, but also have learned that the facts held in them earlier are no longer valid -or have reduced confidence -due to conflicting input data.": [[0, 4, "DATASET"], [42, 46, "DATASET"]], "Unlike WT10g, the queries used with WBR were extracted from a unique search engine query log and represent realistic user behavior which impacts particularly on transactional queries.": [[7, 12, "DATASET"]], "Examples of such knowledge bases include YAGO , DBpedia  and Freebase .": [], "For our sentence selector, we simply employed default parameters trained on data from previous DUC evaluations.": [], "We can see that the 2-seg-phrase approach reduces more than 80% of the processing time on the GOV2 dataset.": [[94, 98, "DATASET"]], "The rule in YAGO format will be like $0, hasWonPrize, $1 \u21d2 $0, participatedIn, $1 If the user runs the Rule Extractor, this will induce the fact Ronaldo, participatedIn, FIFA Confederations Cup .": [], "In the Sixth NTCIR Workshop (NTCIR-6), the Patent Retrieval Task was organized and three subtasks were performed; Japanese Retrieval, English Retrieval, and Classification  1 .": [], "The query distribution with different number of relevant documents of MQ2007 is shown in , and the results in terms of various difficulty levels are shown in .": [[70, 76, "DATASET"]], "On the one hand side, we are considering a preprocessing step that can detect complex (especially YAGO) categories before parsing the natural language question.": [], "Invalidity search, which has newly emerged in patent retrieval, is to retrieve published or registered patents that contain some conflicting claim parts that are enough to  There have been some attempts  to search patent documents before handling patent documents in NTCIR workshops.": [], "The performance of this approach on the DUC 2002 MultiDocument Summarization tasks shows statistically significant improvement over other summarization algorithms in terms of the ROUGE-1 recall measures.": [], "The news articles of RCV2 are categorized along a class hierarchy of 104 overlapping topics.": [[21, 25, "DATASET"]], "While the DUC corpus provides reference summaries for evaluation, no such reference summaries were available for the INEX collection.": [[117, 121, "DATASET"]], "We evince the relationship between IDF and information distance to connect it with MED.": [], "Experiments have been performed on 3 datasets: MovieLens (1M ratings), Vodkaster (2M), Netflix (2M), divided into 3 temporal (chronologically ordered) splits: Train, Dev (20k), Test (20k); the development set is used to tune the different parameters of the algorithm.": [[47, 56, "DATASET"], [71, 80, "DATASET"], [87, 94, "DATASET"]], "In each retrieval task, we first tune the Dirichlet smoothing parameter in Equation 5 to obtain the best QL baseline that can achieve the highest MAP with training queries on each searched target collection (GOV2 or ClueWeb09-T09B).": [[208, 212, "DATASET"], [216, 230, "DATASET"]], "In our experimental study, we use publicly available learning to rank datasets MQ2007 and MQ2008  and a widely accepted protocol for simulating user clicks .": [[79, 85, "DATASET"], [90, 96, "DATASET"]], "On MQ2008, experimental results obtained by applying STL and CTL are also shown in .": [[3, 9, "DATASET"]], "The ranked sentences were added to a summary one by one until the summary exceeded 250 words which was the limit in the DUC evaluations.": [], "Extensive experiments were conducted to test LaP against BOW counterparts, on RCV1 benchmark as well as Yahoo!": [[78, 82, "DATASET"]], "We use four TREC collections: disk12, Robust04, WT2G and Terabyte (GOV2) to conduct the experiments.": [[30, 36, "DATASET"], [38, 46, "DATASET"], [48, 52, "DATASET"], [57, 65, "DATASET"]], "(3) YAGO is a workload of entity labels from the YAGO2 knowledge base .": [], "Evaluation on several text classification tasks on the RCV1 corpus shows the usefulness of our method.": [[55, 59, "DATASET"]], "We reused knowledge-based features and extended data-driven features for CLQA, as described below.": [], "Fortunately, YAGO comes with a simple but powerful heuristics for the preferred meaning of a name: the entity which most frequently occurs in Wikipedia as a link target for an href anchor text with the given name.": [], "In addition, query expansion and cluster-based retrieval are compared, and their combinations are evaluated in terms of retrieval performance by performing experimentations on seven test collections of NTCIR and TREC.": [[202, 207, "DATASET"], [212, 216, "DATASET"]], "We created six-eld documents in the case of GOV2 collection, ve-eld document for each entity in DBpedia-v2 collection, and three-eld document for each product in HomeDepot collection.": [[44, 48, "DATASET"], [96, 106, "DATASET"]], "We report on signi cant improvements the performance on real-world datasets like Freebase or YAGO.": [[81, 89, "DATASET"], [93, 97, "DATASET"]], "However, CePS cannot find the persons in YAGO with similar relations as the two querying persons.": [], "These experiences from RCV1 demonstrate that categorising a general purpose information document collection such as the ClueWeb12 will be a daunting task for humans.": [[23, 27, "DATASET"], [120, 129, "DATASET"]]}